{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture (p.28 Figure 1.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1) # (in channel, out channel, kernel size, stride) ref: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10) # P: probability of an element to be zero-ed. ref: https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html?highlight=dropout2d#torch.nn.Dropout2d\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32. input image should be 12x12.\n",
    "        self.fc2 = nn.Linear(64, 10) # output is {0,..,9}\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2) # (input, kernel_size. ref: https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html?highlight=max_pool2d#torch.nn.functional.max_pool2d\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train() #  sets the mode to train, not eval.\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad() # use stochastic gradient descent, so reset gradient to 0.\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # print\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch. ref: https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # prediction results for a batch. use argmax to get the most likely prediction. keepdim (bool) â€“ whether the output tensor has dim retained or not.ref: https://pytorch.org/docs/stable/generated/torch.argmax.html\n",
    "            success += pred.eq(y.view_as(pred)).sum().item() # view_as: View this tensor as the same size as other. ref: https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7554/3487573542.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# debug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# # debug\n",
    "# for X, y in test_dataloader:\n",
    "#     X, y = X.to(device), y.to(device)\n",
    "#     pred_prob = model(X)\n",
    "#     pred = pred_prob.argmax(dim=1, keepdim=True) \n",
    "#     print(pred)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256 = 0.1302. and train_X.std()/256 = 0.3069.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet() # defined in the above.\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masahiro/software/anaconda3/envs/study/lib/python3.9/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.301563\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.999758\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.404254\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 1.206638\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.798878\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.854381\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.916494\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.722387\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 1.117116\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.958758\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.912771\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.748333\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.910245\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.555541\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.889906\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.994307\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.876377\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.860759\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.770216\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.518240\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.948464\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.471647\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.887609\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.584873\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.396051\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.582964\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 1.017463\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.782979\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.503262\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.581872\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.811795\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.702954\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.465845\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.917504\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.729302\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.559640\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.647431\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.792844\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.358556\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.882026\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.744023\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.601249\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.418895\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.609339\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.773622\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.747383\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.471483\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.864830\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.389010\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.388336\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.286765\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.717156\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.811014\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.745676\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 1.004344\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.609896\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.737626\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.297920\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.774625\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.855755\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.183359\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.504795\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.673686\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.944646\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.563088\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.512679\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.996926\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.525618\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.787118\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.444868\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.775038\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.687720\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.727555\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.838270\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.816093\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.702486\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.862232\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.480723\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.441141\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.646654\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.619225\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 1.244125\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.622559\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.809133\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.658537\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.460459\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.779223\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.703274\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.721577\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.626645\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.469928\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.954540\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.812385\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.406916\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 1.204441\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.395732\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.657533\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.724858\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.809555\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.728017\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.437792\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.472764\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.479086\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.431928\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.455947\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.619440\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.654017\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.588860\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.641847\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.707335\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.759896\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.767821\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.358028\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.500412\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.417460\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.509135\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.599097\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.828675\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.870754\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.735105\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.447469\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 1.050304\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.322583\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.700934\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.795953\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.699381\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.469251\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.706444\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.875864\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.775884\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 1.126254\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.437548\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.583923\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.592903\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.381960\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.754110\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.927753\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.725115\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.487730\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.684915\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.784356\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.935867\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.615964\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.433395\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.696520\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.521962\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.464798\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.591940\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.661632\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 1.151854\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.569794\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.646411\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.942298\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.368334\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 1.094839\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.806674\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.471538\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.874466\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.502482\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.939768\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.918374\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.576047\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.651055\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.891374\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.653531\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.871035\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.436231\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.607222\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.821363\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.445304\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.867544\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.795739\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.220198\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.448796\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.663120\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.636172\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.756809\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.383357\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.594938\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.633859\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.574993\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.598102\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.458649\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.579756\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.478445\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.418802\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.624865\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.741505\n",
      "\n",
      "Test dataset: Overall Loss: 0.0521, Overall Accuracy: 9832/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.672050\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.318790\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.716597\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.624716\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.368793\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.506787\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.575554\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.591099\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.484289\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.361672\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.302677\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.576274\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.938122\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.510998\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.655394\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.668256\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.364848\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.775623\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.575763\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.437106\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.611155\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.231660\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.606745\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.613015\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.581165\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.529868\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.741385\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.936925\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.658173\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.714914\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.790156\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.658632\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.443011\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.790275\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.476277\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.440167\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.321916\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.804387\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.678199\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.588565\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.576548\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.669007\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.857849\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.726361\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.362879\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.722931\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.875510\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.717463\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.728842\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.545114\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.592889\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.689576\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.502530\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.880190\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.797781\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.361623\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.374614\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.599805\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.580896\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.671244\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.366141\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.711501\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.724162\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.802871\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.366773\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.795295\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.904854\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.401525\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.374717\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.724526\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.646247\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.864390\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.599098\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.453003\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.846685\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.511812\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.573664\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.469801\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.723031\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.585601\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.493488\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.798019\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.500481\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.591627\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.312599\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.345174\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.673182\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 1.167754\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 1.078353\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.423322\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.645142\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.362205\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 1.061064\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.545257\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.795333\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.504012\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.538641\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.719060\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.737582\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.792889\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.579604\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.359358\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.865489\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.794450\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.666164\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.693404\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.375346\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.713279\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.360053\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.362614\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.435131\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.920976\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.637777\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.430895\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.583949\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.648270\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.792146\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.510911\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.744932\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.292581\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.725532\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.751673\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.472308\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.230965\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.572634\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.579603\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.937925\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.982660\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.596740\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.667483\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.784225\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.363185\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.568764\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.803445\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.573991\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.359418\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.732735\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.653670\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.987064\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.526845\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.864721\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.737502\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.426732\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.798218\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.711151\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.788274\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.601823\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.546484\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.795361\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.442899\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.768192\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.714132\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.832565\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.789290\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.642976\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.508565\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.502609\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.650078\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.641604\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.926896\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.583499\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.742221\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.679231\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.733531\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.615981\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.732093\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.646262\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.711390\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.672465\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.788652\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 1.065859\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.758329\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.691968\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.516677\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.873676\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.758722\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.218604\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.811718\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.724128\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.718964\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.404005\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.520997\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.612550\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.718871\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.511058\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.661017\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.703646\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.589100\n",
      "\n",
      "Test dataset: Overall Loss: 0.0480, Overall Accuracy: 9841/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(sample_data[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.6153e+01, -1.8396e+01, -1.4071e+01, -1.4373e+01, -2.0294e+01,\n",
       "        -1.9060e+01, -2.6017e+01, -2.1458e-06, -1.7734e+01, -1.4211e+01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_data).data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([-2.1458e-06, -2.0027e-05, -5.0818e-04, -1.3684e-04, -4.5075e-04,\n",
       "        -6.7879e-04, -9.2166e-04, -3.8038e-02, -1.8012e-02, -1.4247e-03,\n",
       "        -2.3842e-06, -1.3315e-04, -4.0835e-03, -1.4305e-06, -3.3266e-04,\n",
       "        -1.8452e-04, -2.6605e-03, -1.5974e-05, -3.4083e-01, -2.1100e-05,\n",
       "        -2.0712e-02, -1.6810e-02, -1.7038e-03, -8.8214e-06, -1.6271e-04,\n",
       "        -6.3537e-05, -2.8892e-04, -4.5299e-06, -2.5987e-05, -1.6178e-03,\n",
       "        -1.7641e-04, -4.9533e-02, -2.9087e-05, -5.7958e-02, -3.5643e-05,\n",
       "        -3.0636e-05, -4.1333e-04, -2.9205e-03, -2.3386e-04, -1.3864e-03,\n",
       "        -3.3120e-03, -1.8670e-03, -7.6634e-04, -8.6863e-03, -2.3983e-02,\n",
       "        -3.9081e-04, -3.9311e-03, -4.9828e-05, -9.2621e-05, -1.3243e-04,\n",
       "        -4.6362e-04, -6.0302e-04, -3.4571e-06, -2.1229e-04, -4.5134e-04,\n",
       "        -7.7486e-06, -7.2717e-06, -5.7609e-04, -6.4264e-03, -1.4100e-02,\n",
       "        -1.0013e-04, -6.1541e-04, -3.7899e-01, -6.1241e-02, -2.5630e-05,\n",
       "        -5.5894e-04, -2.3351e-03, -6.0199e-05, -1.3768e-04, -5.7815e-05,\n",
       "        -1.7762e-05, -3.5763e-07, -1.4030e-04, -7.0003e-02, -3.5320e-03,\n",
       "        -2.3842e-05, -5.6861e-05, -2.0263e-03, -9.9018e-02, -3.0453e-04,\n",
       "        -2.5198e-02, -1.0204e-03, -6.9141e-06, -3.0875e-05, -1.4554e-04,\n",
       "        -1.3947e-05, -6.7351e-05, -1.6745e-02, -1.1563e-04, -9.2071e-04,\n",
       "        -3.4809e-05, -6.9463e-04, -3.9393e-01, -2.7386e-02, -1.6230e-03,\n",
       "        -2.5405e-03, -1.8450e-01, -1.4882e-02, -4.6631e-03, -8.2411e-04,\n",
       "        -4.6517e-04, -4.4107e-06, -1.3709e-05, -2.1458e-06, -3.1247e-03,\n",
       "        -2.2533e-03, -1.0609e-04, -2.3620e-02, -1.6106e-02, -1.7881e-06,\n",
       "        -1.8000e-05, -1.3693e-02, -2.5591e-04, -1.5328e-03, -6.7509e-04,\n",
       "        -6.9217e-02, -8.1887e-04, -7.2238e-05, -1.5795e-02, -3.8833e-03,\n",
       "        -1.9262e-04, -3.6009e-03, -1.8392e-04, -7.2119e-05, -2.7547e-02,\n",
       "        -1.6623e-01, -3.0632e-03, -9.5367e-06, -3.3379e-06, -7.1526e-07,\n",
       "        -5.5345e-04, -4.2334e-04, -2.3484e-05, -6.7009e-04, -2.5034e-06,\n",
       "        -2.5579e-04, -2.1219e-05, -1.1898e-03, -2.8610e-05, -1.0815e-02,\n",
       "        -1.1945e-03, -8.5830e-06, -1.1715e-02, -5.3237e-04, -4.9351e-05,\n",
       "        -2.6157e-03, -1.4305e-05, -3.9934e-05, -3.2186e-06, -2.6213e-02,\n",
       "        -2.1878e-03, -5.0576e-03, -1.2040e-05, -9.4409e-05, -3.1771e-03,\n",
       "        -7.3909e-06, -3.2741e-04, -4.4822e-05, -7.0834e-01, -4.0773e-04,\n",
       "        -1.6211e-02, -4.8173e-04, -1.1921e-07, -4.8040e-05, -4.4193e-04,\n",
       "        -2.3722e-05, -4.1723e-06, -1.5588e-03, -5.0306e-04, -9.6559e-06,\n",
       "        -1.2398e-05, -6.1848e-03, -2.5531e-04, -2.7963e-04, -2.1338e-05,\n",
       "        -3.5948e-02, -1.5397e-03, -1.8750e-04, -1.9846e-04, -2.7367e-04,\n",
       "        -7.8064e-04, -1.5974e-05, -3.3063e-04, -1.4305e-06, -9.3599e-02,\n",
       "        -6.0130e-02, -2.6941e-05, -4.9232e-05, -1.7881e-06, -2.2247e-02,\n",
       "        -6.1202e-03, -1.6768e-02, -3.3378e-05, -7.1588e-03, -8.3205e-05,\n",
       "        -7.2270e-02, -2.2041e-03, -4.1000e-03, -7.0689e-05, -6.9111e-03,\n",
       "        -5.9893e-03, -1.0752e-04, -2.7450e-04, -7.1921e-03, -5.8026e-04,\n",
       "        -1.1348e-04, -9.5958e-03, -5.5274e-04, -1.3219e-04, -1.9043e-01,\n",
       "        -4.3034e-05, -2.8219e-02, -7.0511e-04, -5.1092e-03, -2.1393e-02,\n",
       "        -5.8836e-04, -1.6771e-04, -1.2314e-01, -4.1723e-06, -1.7881e-05,\n",
       "        -2.3784e-03, -2.0324e-03, -6.3876e-04, -5.7576e-05, -2.0583e-03,\n",
       "        -1.5162e-04, -8.8688e-05, -2.9560e-04, -3.6757e-04, -7.4776e-04,\n",
       "        -4.9973e-03, -7.0237e-04, -2.7760e-03, -6.5961e-04, -4.0273e-04,\n",
       "        -1.4900e-03, -1.4901e-05, -4.5563e-04, -4.6492e-06, -5.0020e-04,\n",
       "        -1.5842e-04, -5.9331e-03, -1.8199e-03, -2.4169e-03, -6.0335e-03,\n",
       "        -4.0526e-02, -8.1062e-06, -3.9315e-01, -4.5179e-05, -2.2657e-03,\n",
       "        -1.0109e-03, -3.4980e-02, -7.1708e-03, -3.3497e-05, -9.2502e-05,\n",
       "        -6.1904e-03, -1.3195e-03, -2.0546e-03, -2.9206e-05, -5.8593e-01,\n",
       "        -2.1458e-06, -7.9033e-05, -3.6955e-06, -9.3217e-05, -3.9355e-01,\n",
       "        -3.8307e-04, -3.3295e-01, -3.1109e-04, -1.2943e-02, -2.3184e-04,\n",
       "        -5.4917e-04, -5.1378e-05, -8.1268e-04, -8.9454e-03, -5.9789e-04,\n",
       "        -3.6138e-04, -1.1396e-04, -1.6771e-04, -4.8994e-05, -6.5841e-04,\n",
       "        -3.2598e-04, -1.3780e-04, -2.1076e-02, -1.6689e-05, -6.6032e-03,\n",
       "        -2.1458e-06, -4.4985e-02, -1.2778e-04, -1.5398e-03, -2.4705e-03,\n",
       "        -1.9858e-01, -3.6869e-03, -8.0561e-02, -6.5801e-05, -3.9339e-06,\n",
       "        -2.6226e-05, -7.1526e-07, -2.5448e-04, -9.8462e-05, -1.9073e-06,\n",
       "        -3.4014e-03, -1.4418e-03, -3.3528e-04, -1.6304e-03, -1.6689e-05,\n",
       "        -3.5566e-04, -3.1709e-05, -4.6707e-04, -8.0517e-04, -8.6304e-05,\n",
       "        -9.7985e-05, -2.3246e-05, -4.6252e-05, -1.9578e-01, -3.0859e-04,\n",
       "        -1.2612e-04, -8.2794e-03, -3.5447e-04, -3.5908e-03, -7.7486e-06,\n",
       "        -3.7450e-01, -6.4250e-02, -1.6641e-01, -2.2292e-05, -4.1436e-03,\n",
       "        -1.0359e-03, -4.7786e-02, -1.8358e-05, -2.0037e-04, -3.7055e-04,\n",
       "        -5.6692e-04, -3.1471e-05, -3.0441e-04, -1.1548e-03, -1.3828e-05,\n",
       "        -3.9100e-05, -5.8085e-02, -3.8906e-03, -4.7006e-03, -3.0689e-02,\n",
       "        -6.6493e-02, -8.5089e-02, -5.1092e-04, -4.5909e-04, -4.0256e-03,\n",
       "        -2.2422e-03, -8.7019e-05, -1.1229e-04, -7.1476e-04, -2.0810e-02,\n",
       "        -7.4640e-03, -1.5259e-05, -1.5901e-04, -1.2937e-03, -1.8690e-04,\n",
       "        -5.7478e-04, -2.2650e-06, -1.5025e-02, -3.6530e-02, -4.4885e-02,\n",
       "        -1.2230e-04, -7.6289e-04, -8.9069e-04, -1.7097e-03, -6.0797e-06,\n",
       "        -1.4663e-05, -2.8413e-02, -2.1457e-05, -8.4288e-02, -1.1221e-03,\n",
       "        -1.2397e-04, -1.2040e-05, -3.4863e-04, -2.9325e-05, -7.4966e-04,\n",
       "        -6.0797e-06, -2.4566e-04, -3.3513e-03, -1.4435e-04, -4.3749e-05,\n",
       "        -1.3970e-04, -5.8520e-01, -1.2659e-04, -1.4090e-04, -2.7179e-05,\n",
       "        -1.8380e-04, -1.7007e-02, -1.4149e-04, -5.4716e-05, -1.6300e-02,\n",
       "        -1.0877e-03, -1.8460e-03, -2.4316e-04, -3.6490e-03, -6.6632e-02,\n",
       "        -6.7232e-05, -9.7389e-05, -4.1321e-04, -2.6941e-05, -5.7696e-05,\n",
       "        -2.2153e-02, -4.6491e-05, -3.3373e-04, -4.0225e-04, -5.3523e-02,\n",
       "        -3.0517e-05, -2.5657e-03, -2.5034e-06, -1.1495e-03, -1.7250e-02,\n",
       "        -7.7486e-06, -8.7871e-03, -2.3794e-02, -7.2834e-05, -8.2135e-02,\n",
       "        -1.1685e-03, -2.8535e-04, -3.9941e-02, -8.0582e-05, -5.6847e-04,\n",
       "        -9.4085e-03, -9.3559e-04, -1.5531e-02, -1.4186e-05, -5.0068e-06,\n",
       "        -2.7179e-05, -1.4881e-03, -1.2676e-03, -1.9858e-04, -3.3379e-06,\n",
       "        -4.4372e-04, -2.8194e-02, -4.7346e-03, -2.6703e-05, -2.2785e-03,\n",
       "        -1.3960e-01, -1.1765e-04, -4.8159e-05, -7.2965e-04, -6.2695e-03,\n",
       "        -9.2983e-06, -2.5531e-04, -1.1373e-03, -3.9157e-03, -3.5166e-02,\n",
       "        -2.4091e-01, -6.4173e-03, -1.3899e-01, -4.2901e-02, -5.8161e-03,\n",
       "        -6.0540e-01, -2.9147e-03, -1.1919e-02, -4.6492e-06, -1.9750e-03,\n",
       "        -9.1427e-04, -1.1083e-02, -2.6667e-02, -7.1526e-07, -5.2451e-05,\n",
       "        -5.0633e-03, -1.3982e-03, -2.1433e-01, -1.4149e-04, -3.7852e-01,\n",
       "        -2.6950e-04, -4.1723e-06, -5.6179e-04, -1.1176e-03, -2.1956e-04,\n",
       "        -1.6128e-04, -2.1986e-02, -1.7820e-04, -5.3804e-03, -4.7206e-05,\n",
       "        -5.4477e-05, -5.0437e-04, -3.7401e-04, -7.0559e-04, -8.2261e-03,\n",
       "        -6.4269e-04, -2.0058e-03, -3.5881e-05, -7.5848e-04, -4.2676e-05,\n",
       "        -5.3404e-05, -4.1667e-04, -1.2767e-02, -1.7818e-03, -4.3371e-04,\n",
       "        -3.2332e-02, -4.1107e-04, -2.2381e-02, -7.1526e-07, -2.8610e-06,\n",
       "        -7.4941e-01, -2.0642e-03, -3.2891e-02, -5.5739e-04, -4.1909e-03]),\n",
       "indices=tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 5, 4, 9, 6, 6, 5,\n",
       "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
       "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
       "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 4, 3, 1, 4,\n",
       "        1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2,\n",
       "        5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1,\n",
       "        7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 2, 4, 4, 6, 5, 4, 6, 5, 4, 5,\n",
       "        1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1,\n",
       "        0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0,\n",
       "        3, 6, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1,\n",
       "        5, 9, 8, 7, 2, 3, 0, 4, 4, 2, 4, 1, 9, 5, 7, 7, 2, 8, 2, 6, 8, 5, 7, 7,\n",
       "        9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2, 6, 4,\n",
       "        1, 5, 8, 2, 9, 2, 0, 4, 0, 0, 2, 8, 4, 7, 1, 2, 4, 0, 2, 7, 4, 3, 3, 0,\n",
       "        0, 3, 1, 9, 6, 5, 2, 5, 9, 7, 9, 3, 0, 4, 2, 0, 7, 1, 1, 2, 1, 5, 3, 3,\n",
       "        9, 7, 8, 6, 5, 6, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5, 5, 6, 1, 8, 5, 1, 7, 4,\n",
       "        4, 6, 2, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8, 8, 5, 4, 1, 1, 4, 0, 3, 3, 7,\n",
       "        6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5, 2, 5, 4, 4, 2, 8, 3, 8, 2, 4, 5, 0,\n",
       "        3, 1, 7, 7, 5, 7, 9, 7, 1, 9, 2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8,\n",
       "        4, 5, 9, 8, 8, 3, 7, 6, 0, 0, 3, 0, 2, 0, 6, 4, 9, 5, 3, 3, 2, 3, 9, 1,\n",
       "        2, 6, 8, 0, 5, 6, 6, 6, 3, 8, 8, 2, 7, 5, 8, 9, 6, 1, 8, 4, 1, 2, 5, 9,\n",
       "        1, 9, 7, 5, 4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 8, 9, 4, 0, 6]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample_data).data.max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from here. 2022/8/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
