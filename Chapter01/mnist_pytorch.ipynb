{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture (p.28 Figure 1.19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1) # (in channel, out channel, kernel size, stride) ref: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10) # P: probability of an element to be zero-ed. ref: https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html?highlight=dropout2d#torch.nn.Dropout2d\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32. input image should be 12x12.\n",
    "        self.fc2 = nn.Linear(64, 10) # output is {0,..,9}\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2) # (input, kernel_size. ref: https://pytorch.org/docs/stable/generated/torch.nn.functional.max_pool2d.html?highlight=max_pool2d#torch.nn.functional.max_pool2d\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train() #  sets the mode to train, not eval.\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad() # use stochastic gradient descent, so reset gradient to 0.\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # print\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch. ref: https://pytorch.org/docs/stable/generated/torch.nn.functional.nll_loss.html\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # prediction results for a batch. use argmax to get the most likely prediction. keepdim (bool) â€“ whether the output tensor has dim retained or not.ref: https://pytorch.org/docs/stable/generated/torch.argmax.html\n",
    "            success += pred.eq(y.view_as(pred)).sum().item() # view_as: View this tensor as the same size as other. ref: https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [4],\n",
      "        [9],\n",
      "        [5],\n",
      "        [9],\n",
      "        [0],\n",
      "        [6],\n",
      "        [9],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [9],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [9],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [7],\n",
      "        [4],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [7],\n",
      "        [2],\n",
      "        [7],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2],\n",
      "        [3],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [9],\n",
      "        [5],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9],\n",
      "        [3],\n",
      "        [7],\n",
      "        [4],\n",
      "        [6],\n",
      "        [4],\n",
      "        [3],\n",
      "        [0],\n",
      "        [7],\n",
      "        [0],\n",
      "        [2],\n",
      "        [9],\n",
      "        [1],\n",
      "        [7],\n",
      "        [3],\n",
      "        [2],\n",
      "        [9],\n",
      "        [7],\n",
      "        [7],\n",
      "        [6],\n",
      "        [2],\n",
      "        [7],\n",
      "        [8],\n",
      "        [4],\n",
      "        [7],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [4],\n",
      "        [3],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [7],\n",
      "        [6],\n",
      "        [9],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [9],\n",
      "        [9],\n",
      "        [2],\n",
      "        [1],\n",
      "        [9],\n",
      "        [4],\n",
      "        [8],\n",
      "        [7],\n",
      "        [3],\n",
      "        [9],\n",
      "        [7],\n",
      "        [4],\n",
      "        [4],\n",
      "        [4],\n",
      "        [9],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [7],\n",
      "        [6],\n",
      "        [7],\n",
      "        [9],\n",
      "        [0],\n",
      "        [5],\n",
      "        [8],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [7],\n",
      "        [8],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [6],\n",
      "        [7],\n",
      "        [3],\n",
      "        [1],\n",
      "        [7],\n",
      "        [1],\n",
      "        [8],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [9],\n",
      "        [9],\n",
      "        [5],\n",
      "        [5],\n",
      "        [1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [5],\n",
      "        [1],\n",
      "        [4],\n",
      "        [4],\n",
      "        [7],\n",
      "        [2],\n",
      "        [3],\n",
      "        [2],\n",
      "        [7],\n",
      "        [1],\n",
      "        [8],\n",
      "        [1],\n",
      "        [8],\n",
      "        [1],\n",
      "        [8],\n",
      "        [5],\n",
      "        [0],\n",
      "        [8],\n",
      "        [9],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [9],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [3],\n",
      "        [9],\n",
      "        [5],\n",
      "        [2],\n",
      "        [9],\n",
      "        [4],\n",
      "        [5],\n",
      "        [9],\n",
      "        [3],\n",
      "        [9],\n",
      "        [0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [5],\n",
      "        [7],\n",
      "        [2],\n",
      "        [2],\n",
      "        [7],\n",
      "        [1],\n",
      "        [2],\n",
      "        [8],\n",
      "        [4],\n",
      "        [1],\n",
      "        [7],\n",
      "        [3],\n",
      "        [3],\n",
      "        [8],\n",
      "        [8],\n",
      "        [7],\n",
      "        [9],\n",
      "        [2],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [9],\n",
      "        [8],\n",
      "        [7],\n",
      "        [2],\n",
      "        [3],\n",
      "        [0],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [4],\n",
      "        [1],\n",
      "        [9],\n",
      "        [5],\n",
      "        [7],\n",
      "        [7],\n",
      "        [2],\n",
      "        [8],\n",
      "        [2],\n",
      "        [6],\n",
      "        [8],\n",
      "        [5],\n",
      "        [7],\n",
      "        [7],\n",
      "        [9],\n",
      "        [1],\n",
      "        [8],\n",
      "        [1],\n",
      "        [8],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [1],\n",
      "        [9],\n",
      "        [9],\n",
      "        [4],\n",
      "        [1],\n",
      "        [8],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [9],\n",
      "        [7],\n",
      "        [5],\n",
      "        [9],\n",
      "        [2],\n",
      "        [6],\n",
      "        [4],\n",
      "        [1],\n",
      "        [5],\n",
      "        [8],\n",
      "        [2],\n",
      "        [9],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [8],\n",
      "        [4],\n",
      "        [7],\n",
      "        [1],\n",
      "        [2],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [7],\n",
      "        [4],\n",
      "        [3],\n",
      "        [3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [9],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [9],\n",
      "        [7],\n",
      "        [9],\n",
      "        [3],\n",
      "        [0],\n",
      "        [4],\n",
      "        [2],\n",
      "        [0],\n",
      "        [7],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [3],\n",
      "        [9],\n",
      "        [7],\n",
      "        [8],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [1],\n",
      "        [3],\n",
      "        [8],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [1],\n",
      "        [8],\n",
      "        [5],\n",
      "        [1],\n",
      "        [7],\n",
      "        [4],\n",
      "        [4],\n",
      "        [6],\n",
      "        [2],\n",
      "        [2],\n",
      "        [5],\n",
      "        [0],\n",
      "        [6],\n",
      "        [5],\n",
      "        [6],\n",
      "        [3],\n",
      "        [7],\n",
      "        [2],\n",
      "        [0],\n",
      "        [8],\n",
      "        [8],\n",
      "        [5],\n",
      "        [4],\n",
      "        [1],\n",
      "        [1],\n",
      "        [4],\n",
      "        [0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [7],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [2],\n",
      "        [1],\n",
      "        [9],\n",
      "        [2],\n",
      "        [8],\n",
      "        [6],\n",
      "        [1],\n",
      "        [9],\n",
      "        [5],\n",
      "        [2],\n",
      "        [5],\n",
      "        [4],\n",
      "        [4],\n",
      "        [2],\n",
      "        [8],\n",
      "        [3],\n",
      "        [8],\n",
      "        [2],\n",
      "        [4],\n",
      "        [5],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [7],\n",
      "        [7],\n",
      "        [5],\n",
      "        [7],\n",
      "        [9],\n",
      "        [7],\n",
      "        [1],\n",
      "        [9],\n",
      "        [2],\n",
      "        [1],\n",
      "        [4],\n",
      "        [2],\n",
      "        [9],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4],\n",
      "        [9],\n",
      "        [1],\n",
      "        [4],\n",
      "        [8],\n",
      "        [1],\n",
      "        [8],\n",
      "        [4],\n",
      "        [5],\n",
      "        [9],\n",
      "        [8],\n",
      "        [8],\n",
      "        [3],\n",
      "        [7],\n",
      "        [6],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [6],\n",
      "        [4],\n",
      "        [9],\n",
      "        [5],\n",
      "        [3],\n",
      "        [3],\n",
      "        [2],\n",
      "        [3],\n",
      "        [9],\n",
      "        [1],\n",
      "        [2],\n",
      "        [6],\n",
      "        [8],\n",
      "        [0],\n",
      "        [5],\n",
      "        [6],\n",
      "        [6],\n",
      "        [6],\n",
      "        [3],\n",
      "        [8],\n",
      "        [8],\n",
      "        [2],\n",
      "        [7],\n",
      "        [5],\n",
      "        [8],\n",
      "        [9],\n",
      "        [6],\n",
      "        [1],\n",
      "        [8],\n",
      "        [4],\n",
      "        [1],\n",
      "        [2],\n",
      "        [5],\n",
      "        [9],\n",
      "        [1],\n",
      "        [9],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [0],\n",
      "        [8],\n",
      "        [9],\n",
      "        [9],\n",
      "        [1],\n",
      "        [0],\n",
      "        [5],\n",
      "        [2],\n",
      "        [3],\n",
      "        [7],\n",
      "        [8],\n",
      "        [9],\n",
      "        [4],\n",
      "        [0],\n",
      "        [6]])\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "for X, y in test_dataloader:\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    pred_prob = model(X)\n",
    "    pred = pred_prob.argmax(dim=1, keepdim=True) \n",
    "    print(pred)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from here. 2022/8/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4df638a7d1487bbe610bce30680ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dca9441a36a4e84b486beb080a1bce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e39640dc62e45fbbb3876eee51e1754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def42872570649178c0a26881cf359be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/masahiro/software/anaconda3/envs/study/lib/python3.9/site-packages/torch/nn/functional.py:1320: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.301563\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.999758\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.404254\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 1.206638\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.798878\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.854381\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.916494\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.722387\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 1.117116\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.958758\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.912771\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.748333\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.910245\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.555541\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.889906\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.994307\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.876377\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.860759\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.770216\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.518240\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.948464\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.471647\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.887609\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.584873\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.396051\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.582964\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 1.017463\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.782979\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.503262\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.581872\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.811795\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.702954\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.465845\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.917504\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.729302\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.559640\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.647431\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.792844\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.358556\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.882026\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.744023\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.601249\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.418895\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.609339\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.773622\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.747383\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.471483\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.864830\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.389010\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.388336\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.286765\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.717156\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.811014\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.745676\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 1.004344\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.609896\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.737626\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.297920\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.774625\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.855755\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.183359\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.504795\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.673686\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.944646\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.563088\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.512679\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.996926\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.525618\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.787118\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.444868\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.775038\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.687720\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.727555\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.838270\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.816093\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.702486\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.862232\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.480723\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.441141\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.646654\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.619225\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 1.244125\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.622559\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.809133\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.658537\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.460459\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.779223\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.703274\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.721577\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.626645\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.469928\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.954540\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.812385\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.406916\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 1.204441\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.395732\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.657533\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.724858\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.809555\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.728017\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.437792\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.472764\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.479086\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.431928\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.455947\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.619440\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.654017\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.588860\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.641847\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.707335\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.759896\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.767821\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.358028\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.500412\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.417460\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.509135\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.599097\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.828675\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.870754\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.735105\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.447469\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 1.050304\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.322583\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.700934\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.795953\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.699381\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.469251\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.706444\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.875864\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.775884\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 1.126254\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.437548\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.583923\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.592903\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.381960\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.754110\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.927753\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.725115\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.487730\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.684915\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.784356\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.935867\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.615964\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.433395\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.696520\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.521962\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.464798\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.591940\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.661632\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 1.151854\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.569794\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.646411\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.942298\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.368334\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 1.094839\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.806674\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.471538\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.874466\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.502482\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.939768\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.918374\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.576047\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.651055\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.891374\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.653531\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.871035\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.436231\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.607222\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.821363\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.445304\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.867544\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.795739\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.220198\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.448796\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.663120\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.636172\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.756809\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.383357\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.594938\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.633859\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.574993\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.598102\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.458649\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.579756\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.478445\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.418802\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.624865\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.741505\n",
      "\n",
      "Test dataset: Overall Loss: 0.0521, Overall Accuracy: 9832/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.672050\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.318790\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.716597\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.624716\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.368793\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.506787\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.575554\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.591099\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.484289\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.361672\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.302677\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.576274\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.938122\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.510998\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.655394\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.668256\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.364848\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.775623\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.575763\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.437106\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.611155\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.231660\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.606745\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.613015\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.581165\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.529868\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.741385\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.936925\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.658173\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.714914\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.790156\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.658632\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.443011\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.790275\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.476277\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.440167\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.321916\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.804387\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.678199\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.588565\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.576548\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.669007\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.857849\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.726361\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.362879\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.722931\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.875510\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.717463\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.728842\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.545114\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.592889\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.689576\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.502530\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.880190\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.797781\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.361623\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.374614\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.599805\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.580896\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.671244\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.366141\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.711501\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.724162\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.802871\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.366773\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.795295\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.904854\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.401525\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.374717\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.724526\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.646247\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.864390\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.599098\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.453003\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.846685\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.511812\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.573664\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.469801\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.723031\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.585601\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.493488\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.798019\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.500481\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.591627\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.312599\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.345174\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.673182\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 1.167754\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 1.078353\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.423322\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.645142\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.362205\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 1.061064\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.545257\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.795333\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.504012\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.538641\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.719060\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.737582\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.792889\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.579604\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.359358\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.865489\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.794450\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.666164\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.693404\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.375346\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.713279\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.360053\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.362614\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.435131\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.920976\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.637777\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.430895\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.583949\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.648270\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.792146\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.510911\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.744932\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.292581\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.725532\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.751673\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.472308\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.230965\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.572634\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.579603\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.937925\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.982660\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.596740\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.667483\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.784225\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.363185\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.568764\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.803445\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.573991\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.359418\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.732735\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.653670\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.987064\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.526845\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.864721\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.737502\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.426732\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.798218\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.711151\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.788274\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.601823\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.546484\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.795361\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.442899\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.768192\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.714132\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.832565\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.789290\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.642976\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.508565\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.502609\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.650078\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.641604\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.926896\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.583499\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.742221\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.679231\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.733531\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.615981\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.732093\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.646262\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.711390\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.672465\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.788652\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 1.065859\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.758329\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.691968\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.516677\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.873676\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.758722\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.218604\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.811718\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.724128\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.718964\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.404005\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.520997\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.612550\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.718871\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.511058\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.661017\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.703646\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.589100\n",
      "\n",
      "Test dataset: Overall Loss: 0.0480, Overall Accuracy: 9841/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
