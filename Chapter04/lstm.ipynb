{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM \n",
    "### good explanation:  http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(123)\n",
    "\n",
    "import random\n",
    "from torchtext import (data, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FIELD = data.Field(tokenize = data.get_tokenizer(\"basic_english\"), include_lengths = True)\n",
    "# get_tokenizer(): split sentence by spaces. [ref](https://pytorch.org/text/stable/data_utils.html)\n",
    "LABEL_FIELD = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_dataset, test_dataset = datasets.IMDB.splits(TEXT_FIELD, LABEL_FIELD)\n",
    "# IMDB: [Stanford's large movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "train_dataset, valid_dataset = train_dataset.split(random_state = random.seed(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCABULARY_SIZE = 25000\n",
    "\n",
    "TEXT_FIELD.build_vocab(train_dataset, \n",
    "                 max_size = MAX_VOCABULARY_SIZE)\n",
    "\n",
    "LABEL_FIELD.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_SIZE = 64 # batch size\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_iterator, valid_data_iterator, test_data_iterator = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset), \n",
    "    batch_size = B_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are training using GPUs, we need to use the following function for the pack_padded_sequence method to work \n",
    "## (reference : https://discuss.pytorch.org/t/error-with-lengths-in-pack-padded-sequence/35517/3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "\n",
    "def cuda_pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=False):\n",
    "    # length becoms a tensor. ref: https://pytorch.org/docs/stable/generated/torch.as_tensor.html\n",
    "    lengths = torch.as_tensor(lengths, dtype=torch.int64)\n",
    "    lengths = lengths.cpu()\n",
    "    \n",
    "    if enforce_sorted:\n",
    "        sorted_indices = None\n",
    "    else:\n",
    "        # sorted_indices means original position at after sorted tensor.\n",
    "        # ref: https://pytorch.org/docs/stable/generated/torch.sort.html\n",
    "        lengths, sorted_indices = torch.sort(lengths, descending=True)\n",
    "        sorted_indices = sorted_indices.to(input.device)\n",
    "        \n",
    "    batch_dim = 0 if batch_first else 1\n",
    "    input = input.index_select(batch_dim, sorted_indices)\n",
    "\n",
    "    data, batch_sizes = \\\n",
    "    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)\n",
    "    return PackedSequence(data, batch_sizes, sorted_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dimension, hidden_dimension, output_dimension, dropout, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension, padding_idx = pad_index)\n",
    "        # num_layers should be > 1. otherwise, it cause below warning\n",
    "        # UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
    "        self.lstm_layer = nn.LSTM(embedding_dimension, \n",
    "                           hidden_dimension, \n",
    "                           num_layers=2, \n",
    "                           bidirectional=True, \n",
    "                           dropout=dropout)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension * 2, output_dimension)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, sequence, sequence_lengths=None):\n",
    "        if sequence_lengths is None:\n",
    "            sequence_lengths = torch.LongTensor([len(sequence)])\n",
    "        \n",
    "        # sequence := (sequence_length, batch_size)\n",
    "        embedded_output = self.dropout_layer(self.embedding_layer(sequence))\n",
    "        \n",
    "        \n",
    "        # embedded_output := (sequence_length, batch_size, embedding_dimension)\n",
    "        if torch.cuda.is_available():\n",
    "            packed_embedded_output = cuda_pack_padded_sequence(embedded_output, sequence_lengths)\n",
    "        else:\n",
    "            packed_embedded_output = nn.utils.rnn.pack_padded_sequence(embedded_output, sequence_lengths)\n",
    "        \n",
    "        packed_output, (hidden_state, cell_state) = self.lstm_layer(packed_embedded_output)\n",
    "        # hidden_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        # num_directions = 2 if bidirectional LSTM.\n",
    "        # cell_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        \n",
    "        op, op_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # op := (sequence_length, batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        hidden_output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)        \n",
    "        # hidden_output := (batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        return self.fc_layer(hidden_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = len(TEXT_FIELD.vocab)\n",
    "EMBEDDING_DIMENSION = 100\n",
    "HIDDEN_DIMENSION = 32\n",
    "OUTPUT_DIMENSION = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.pad_token]\n",
    "\n",
    "lstm_model = LSTM(INPUT_DIMENSION, \n",
    "            EMBEDDING_DIMENSION, \n",
    "            HIDDEN_DIMENSION, \n",
    "            OUTPUT_DIMENSION, \n",
    "            DROPOUT, \n",
    "            PAD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.unk_token] # unk means unknown\n",
    "\n",
    "lstm_model.embedding_layer.weight.data[UNK_INDEX] = torch.zeros(EMBEDDING_DIMENSION)\n",
    "lstm_model.embedding_layer.weight.data[PAD_INDEX] = torch.zeros(EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lstm_model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss() # binary cross entropy.\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns 0-1 accuracy for the given set of predictions and ground truth\n",
    "    \"\"\"\n",
    "    # round predictions to either 0 or 1\n",
    "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
    "    success = (rounded_predictions == ground_truth).float() #convert into float for division \n",
    "    accuracy = success.sum() / len(success)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_iterator, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    \n",
    "    for curr_batch in data_iterator:\n",
    "        optim.zero_grad()\n",
    "        sequence, sequence_lengths = curr_batch.text\n",
    "        preds = lstm_model(sequence, sequence_lengths).squeeze(1)\n",
    "        \n",
    "        loss_curr = loss_func(preds, curr_batch.label)\n",
    "        accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "        \n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(data_iterator), accuracy/len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_iterator, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for curr_batch in data_iterator:\n",
    "            sequence, sequence_lengths = curr_batch.text\n",
    "            preds = model(sequence, sequence_lengths).squeeze(1)\n",
    "            \n",
    "            loss_curr = loss_func(preds, curr_batch.label)\n",
    "            accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "        \n",
    "    return loss/len(data_iterator), accuracy/len(data_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number: 1 | time elapsed: 14.727524042129517s\n",
      "training loss: 0.683 | training accuracy: 55.39%\n",
      "validation loss: 0.629 |  validation accuracy: 64.82%\n",
      "\n",
      "epoch number: 2 | time elapsed: 18.555153608322144s\n",
      "training loss: 0.637 | training accuracy: 63.47%\n",
      "validation loss: 0.618 |  validation accuracy: 64.90%\n",
      "\n",
      "epoch number: 3 | time elapsed: 18.414896249771118s\n",
      "training loss: 0.597 | training accuracy: 68.32%\n",
      "validation loss: 0.543 |  validation accuracy: 72.93%\n",
      "\n",
      "epoch number: 4 | time elapsed: 19.25935125350952s\n",
      "training loss: 0.570 | training accuracy: 70.77%\n",
      "validation loss: 0.503 |  validation accuracy: 76.57%\n",
      "\n",
      "epoch number: 5 | time elapsed: 19.893606185913086s\n",
      "training loss: 0.513 | training accuracy: 74.96%\n",
      "validation loss: 0.500 |  validation accuracy: 77.48%\n",
      "\n",
      "epoch number: 6 | time elapsed: 18.90823459625244s\n",
      "training loss: 0.472 | training accuracy: 78.17%\n",
      "validation loss: 0.427 |  validation accuracy: 80.81%\n",
      "\n",
      "epoch number: 7 | time elapsed: 19.202368021011353s\n",
      "training loss: 0.437 | training accuracy: 80.17%\n",
      "validation loss: 0.383 |  validation accuracy: 83.70%\n",
      "\n",
      "epoch number: 8 | time elapsed: 20.30467176437378s\n",
      "training loss: 0.411 | training accuracy: 81.90%\n",
      "validation loss: 0.381 |  validation accuracy: 84.08%\n",
      "\n",
      "epoch number: 9 | time elapsed: 19.071581602096558s\n",
      "training loss: 0.385 | training accuracy: 83.12%\n",
      "validation loss: 0.362 |  validation accuracy: 85.28%\n",
      "\n",
      "epoch number: 10 | time elapsed: 18.84856081008911s\n",
      "training loss: 0.355 | training accuracy: 84.73%\n",
      "validation loss: 0.321 |  validation accuracy: 86.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "\n",
    "    time_start = time.time()\n",
    "    \n",
    "    training_loss, train_accuracy = train(lstm_model, train_data_iterator, optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(lstm_model, valid_data_iterator, loss_func)\n",
    "    \n",
    "    time_end = time.time()\n",
    "    time_delta = time_end - time_start \n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(lstm_model.state_dict(), 'lstm_model.pt')\n",
    "    \n",
    "    print(f'epoch number: {ep+1} | time elapsed: {time_delta}s')\n",
    "    print(f'training loss: {training_loss:.3f} | training accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'validation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training result & Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.345 | test accuracy: 85.73%\n"
     ]
    }
   ],
   "source": [
    "lstm_model.load_state_dict(torch.load('../../Mastering-PyTorch/Chapter04/lstm_model.pt'))\n",
    "\n",
    "test_loss, test_accuracy = validate(lstm_model, test_data_iterator, loss_func)\n",
    "\n",
    "print(f'test loss: {test_loss:.3f} | test accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    # text transformations\n",
    "    tokenized = data.get_tokenizer(\"basic_english\")(sentence)\n",
    "    tokenized = [TEXT_FIELD.vocab.stoi[t] for t in tokenized]\n",
    "    \n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    \n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "    \n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1848733276128769\n",
      "0.06664347648620605\n",
      "0.48671120405197144\n",
      "0.7506839632987976\n",
      "0.909300684928894\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(lstm_model, \"This film is horrible\"))\n",
    "print(sentiment_inference(lstm_model, \"Director tried too hard but this film is bad\"))\n",
    "print(sentiment_inference(lstm_model, \"Decent movie, although could be shorter\"))\n",
    "print(sentiment_inference(lstm_model, \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(lstm_model, \"I loved the movie, every part of it\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45216652750968933\n",
      "0.45216652750968933\n",
      "0.7792609930038452\n",
      "0.7865818738937378\n",
      "0.9087597131729126\n",
      "0.7414801120758057\n",
      "0.7132746577262878\n",
      "0.8986361026763916\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(lstm_model, \"hogehoge\"))\n",
    "print(sentiment_inference(lstm_model, \"fugafuga\"))\n",
    "print(sentiment_inference(lstm_model, \"I like it, but my friend hate it.\"))\n",
    "print(sentiment_inference(lstm_model, \"I hate it, but my friend likes it.\"))\n",
    "print(sentiment_inference(lstm_model, \"I love you.\"))\n",
    "print(sentiment_inference(lstm_model, \"I love this movie, by the way I hate the book.\"))\n",
    "print(sentiment_inference(lstm_model, \"I hate this movie, by the way I love the book.\"))\n",
    "print(sentiment_inference(lstm_model, \"With best New Year’s wishes!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to visualize using captum\n",
    "### but it doen't work yet by the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "# [captum](https://captum.ai/): library for model interpretability\n",
    "# [Layer Integrated Gradients](https://github.com/pytorch/captum/blob/master/captum/attr/_core/layer/layer_integrated_gradients.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 636 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: setuptools in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (58.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/anaconda3/envs/study/lib/python3.9/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# [spacy](https://spacy.io/): Library for Natural Language Processing\n",
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(lstm_model, lstm_model.embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_with_sigmoid(input, l):\n",
    "    return torch.sigmoid(lstm_model(input, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(model, sentence, min_len = 7, label = 0):\n",
    "    # text transformations\n",
    "    tokenized = data.get_tokenizer(\"basic_english\")(sentence)\n",
    "    tokenized = [TEXT_FIELD.vocab.stoi[t] for t in tokenized]\n",
    "    \n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    length_input = torch.LongTensor([len(tokenized)])\n",
    "    pred = torch.sigmoid(model(model_input, length_input))\n",
    "\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(len(tokenized), device=device).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    print(f\"model_input.shape={model_input.shape}\")\n",
    "    print(f\"reference_indices.shape={reference_indices.shape}\")\n",
    "    print(f\"reference_indices={reference_indices}\")\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    # below original line is buggy.\n",
    "    attributions_ig, delta = lig.attribute(model_input,\n",
    "                                           reference_indices.reshape(model_input.shape), \n",
    "                                           n_steps=500, return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', Label.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n",
    "    \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            Label.vocab.itos[pred_ind],\n",
    "                            Label.vocab.itos[label],\n",
    "                            Label.vocab.itos[1],\n",
    "                            attributions.sum(),       \n",
    "                            text,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_input.shape=torch.Size([6, 1])\n",
      "reference_indices.shape=torch.Size([1, 6])\n",
      "reference_indices=tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8749/2447905405.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minterpret_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'It was a fantastic performance !'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minterpret_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Best film ever'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minterpret_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Such a great show!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minterpret_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'It was a horrible movie'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minterpret_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'I\\'ve never watched something as bad'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8749/2077781401.py\u001b[0m in \u001b[0;36minterpret_sentence\u001b[0;34m(model, sentence, min_len, label)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# compute attributions and approximation delta using layer integrated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# below original line is buggy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     attributions_ig, delta = lig.attribute(model_input,\n\u001b[0m\u001b[1;32m     25\u001b[0m                                            \u001b[0mreference_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                            n_steps=500, return_convergence_delta=True)\n",
      "\u001b[0;32m~/anaconda3/envs/study/lib/python3.9/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/study/lib/python3.9/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    486\u001b[0m         )\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         attributions = self.ig.attribute.__wrapped__(  # type: ignore\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mig\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0minputs_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/study/lib/python3.9/site-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    283\u001b[0m             )\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             attributions = self._attribute(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/study/lib/python3.9/site-packages/captum/attr/_core/integrated_gradients.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         grads = self.gradient_func(\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mforward_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaled_features_tpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/study/lib/python3.9/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py\u001b[0m in \u001b[0;36mgradient_func\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;31m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# contains batch_size * #steps elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/study/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cudnn RNN backward can only be called in training mode"
     ]
    }
   ],
   "source": [
    "interpret_sentence(lstm_model, 'It was a fantastic performance !', label=1)\n",
    "interpret_sentence(lstm_model, 'Best film ever', label=1)\n",
    "interpret_sentence(lstm_model, 'Such a great show!', label=1)\n",
    "interpret_sentence(lstm_model, 'It was a horrible movie', label=0)\n",
    "interpret_sentence(lstm_model, 'I\\'ve never watched something as bad', label=0)\n",
    "interpret_sentence(lstm_model, 'It is a disgusting movie!', label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
